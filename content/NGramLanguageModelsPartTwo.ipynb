{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See part one [here](./ngram-language-model-part-one.html).\n",
    "#### Generating N-grams\n",
    "\n",
    "First we want a method to quickly generate a list of n-grams given a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iter_ngrams(doc, n):\n",
    "    \"\"\"Return a generator over ngrams of a document.\n",
    "    Params:\n",
    "      doc...list of tokens\n",
    "      n.....size of ngrams\"\"\"\n",
    "    # doc[i: i+n] creates a list of elements from index i to i+n\n",
    "    return (doc[i : i+n] for i in range(len(doc)-n+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a list where each element is a word in a sentence, the iter_ngrams() function returns a generator over the ngrams from the sentence. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'car', 'drove'],\n",
       " ['car', 'drove', 'down'],\n",
       " ['drove', 'down', 'the'],\n",
       " ['down', 'the', 'street.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The car drove down the street.\"\n",
    "\n",
    "list(iter_ngrams(sentence.split(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `sentence` is one single string, I call `.split()` on it to convert it into a list where each element is a word in the sentence.\n",
    "\n",
    "From the dataset, I'll have a dictionary where the key is a name (or ID) and the value is a list of sentences. So for each sentence, I'll need to create its ngrams and then update a count that keeps track of the frequency of each ngram. \n",
    "I'll use a Counter to keep track of the ngram frequencies. A Counter is a special version of a dictionary that is perfect for keeping track of counts (who would've guessed!). \n",
    "\n",
    "The key will be the n-1 words of the ngram and the value will be a Counter dictionary that keeps track of the count of all of the words that follow from the n-1 words. It's easier to see an example of this data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts=\n",
      " (('have', 'you'), Counter({'been': 1, 'slept': 1}))\n",
      "(('you', 'been'), Counter({'today?': 1}))\n",
      "(('how', 'have'), Counter({'you': 1}))\n",
      "(('hello', 'there'), Counter({'friend.': 1}))\n",
      "(('you', 'slept'), Counter({'today?': 1}))\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "names_dict = {}\n",
    "# An example dictionary\n",
    "names_dict['Michael'] = ['hello there friend.', \n",
    "                         'how have you been today?',\n",
    "                         'have you slept today?']\n",
    "\n",
    "sentences = names_dict['Michael']\n",
    "n = 3\n",
    "\n",
    "counts = defaultdict(lambda: Counter())\n",
    "\n",
    "# for each sentence\n",
    "for sentence in sentences:\n",
    "    # convert the sentence into a list of ngrams\n",
    "    # then for each ngram in that list of ngrams, update the count\n",
    "    for ngram in iter_ngrams(sentence.split(), n):\n",
    "        # Create a tuple of the n-1 words and then use this as a key to\n",
    "        # access the Counter. Update the counter using the nth word.\n",
    "        counts[tuple(ngram[:-1])].update([ngram[-1]])\n",
    "        \n",
    "print('counts=\\n', '\\n'.join(str(i) for i in counts.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the sample dataset, `'how have'` was followed by `'you'`. Moreover, `'have you'` occurred twice and was followed by `'slept'` once and `'been'` once. \n",
    "In order to convert these to probabilities, I iterate over all of the counters, sum up the total counts and divide each count by the total. For this small sample dataset, the probability of `'how have'` being followed by `'you'` is 100%. As for `'have you'`, 50% of the time it was followed by `'slept'` and 50% of the time it was followed by `'been'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "counts=\n",
      " (('have', 'you'), {'been': 0.5, 'slept': 0.5})\n",
      "(('you', 'been'), {'today?': 1.0})\n",
      "(('how', 'have'), {'you': 1.0})\n",
      "(('hello', 'there'), {'friend.': 1.0})\n",
      "(('you', 'slept'), {'today?': 1.0})\n"
     ]
    }
   ],
   "source": [
    "for ngram, word_counts in counts.items():\n",
    "    total = sum(word_counts.values())\n",
    "    counts[ngram] = {word: count / total for word, count in word_counts.items()}\n",
    "\n",
    "print('\\ncounts=\\n', '\\n'.join(str(i) for i in counts.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preprocessing\n",
    "\n",
    "There are a few preprocessing steps I'll perform to improve the results. For one, I'll prepend a `\"[s]\"` and append a `\"[/s]\"` to each sentence. When each sentence is generated, the first word will be selected from all of the ngrams that start with `\"[s]\"` and words will be selected until a `\"[/s]\"` is generated. Moreover, words will be transformed to lowercase and spaces will be added before and after each punctuation mark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample ! see ? often , we'll swim ( but only sometimes ) ? ! ? \n"
     ]
    }
   ],
   "source": [
    "sample = \"Sample! See? Often, we'll swim (but only sometimes)?!?\"\n",
    "import re\n",
    "def preprocess(string):\n",
    "    s = re.sub(\"([.,!?()])\", r\" \\1 \", string)\n",
    "    s = re.sub(\"\\s{2,}\", \" \", s)\n",
    "    return s.lower()\n",
    "print(preprocess(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to create the dictionary that will hold all of our preprocessed messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "count = 0\n",
    "names_dict = defaultdict(lambda: [])\n",
    "\n",
    "with open(\"allo_messages_anon.csv\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        sender = row['sender_id']\n",
    "        body = row[\"text\"]\n",
    "        \n",
    "        # IF condition filters out messages with blank text\n",
    "        if body:\n",
    "            names_dict[sender].append(\"[s] \"+ preprocess(body) + \" [/s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['159', '14', '1', '54', '156', '3', '2'])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[s] i'm still recovering from last night lol [/s]\",\n",
       " '[s] same [/s]',\n",
       " '[s] me too !  [/s]',\n",
       " '[s] ^ [/s]',\n",
       " '[s] it is [/s]',\n",
       " '[s] ^^ but totally accurate lol [/s]',\n",
       " '[s] almost done [/s]',\n",
       " \"[s] i'm still working on it [/s]\",\n",
       " \"[s] it's going pretty well [/s]\",\n",
       " '[s] idk , i was thinking monday or tuesday [/s]']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_dict['1'][30:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sender_IDs that I'm interested in are 54, 1, 14, 3, and 2. The remaining two are Allo bots that only have a handful of messages. \n",
    "\n",
    "#### Generating the Language Models\n",
    "\n",
    "Now I'll make a function to generate the ngram probabilities using the code from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_ngram_probs(sentences, n):\n",
    "    \n",
    "    counts = defaultdict(lambda: Counter())\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for ngram in iter_ngrams(sentence.split(), n):\n",
    "            counts[tuple(ngram[:-1])].update([ngram[-1]])\n",
    "            \n",
    "    # Normalize probabilities to sum to 1.0\n",
    "    for ngram, word_counts in counts.items():\n",
    "        total = sum(word_counts.values())\n",
    "        counts[ngram] = {word: count / total for word, count in word_counts.items()}\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'caught': 0.25, 'dead': 0.25, 'just': 0.25, 'welcome': 0.25}\n"
     ]
    }
   ],
   "source": [
    "c = estimate_ngram_probs(names_dict['54'], 3)\n",
    "\n",
    "print(c[('[s]', \"you're\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll create a list of IDs and iterate over it to populate a dictionary with each person's ngram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = ['1', '2', '3', '14', '54']\n",
    "n = 3\n",
    "\n",
    "people_ngrams = {}\n",
    "\n",
    "for id in ids:\n",
    "    people_ngrams[id] = estimate_ngram_probs(names_dict[id], n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the language models! And we have five in particular, where each model represents the word choice tendencies of each of us (including the Google Assitant). I'll leave it to you to try and guess which one is the Google Assistant ;D\n",
    "\n",
    "For now I chose `n=3` (later on we'll explore changing n). So let's look at the results of the language model for sentences that start with \"you're\". The particular trigram for that case will be `('[s]', \"you're\")` followed by the probability of all of the possible following words. So for user '1', 17.64% of the time a message begins with `\"you're\"` it is followed by `'welcome'`. Makes sense! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[/s]': 0.029411764705882353,\n",
       " 'always': 0.029411764705882353,\n",
       " 'amazing': 0.029411764705882353,\n",
       " 'being': 0.029411764705882353,\n",
       " 'doing': 0.029411764705882353,\n",
       " 'ginny': 0.029411764705882353,\n",
       " 'going': 0.08823529411764706,\n",
       " 'gonna': 0.058823529411764705,\n",
       " 'here': 0.029411764705882353,\n",
       " 'just': 0.029411764705882353,\n",
       " 'killin': 0.029411764705882353,\n",
       " 'listening': 0.029411764705882353,\n",
       " 'not': 0.058823529411764705,\n",
       " 'perfect': 0.029411764705882353,\n",
       " 'right': 0.029411764705882353,\n",
       " 'so': 0.11764705882352941,\n",
       " 'the': 0.11764705882352941,\n",
       " 'welcome': 0.17647058823529413,\n",
       " 'welcome😘': 0.029411764705882353}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_ngrams['1'][('[s]', \"you're\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.07317073170731707,\n",
       " 'adorable': 0.024390243902439025,\n",
       " 'awake': 0.024390243902439025,\n",
       " 'cruel': 0.024390243902439025,\n",
       " 'definitely': 0.04878048780487805,\n",
       " 'drunk': 0.04878048780487805,\n",
       " 'exactly': 0.024390243902439025,\n",
       " 'funny': 0.024390243902439025,\n",
       " 'going': 0.024390243902439025,\n",
       " 'gonna': 0.07317073170731707,\n",
       " 'good': 0.024390243902439025,\n",
       " 'just': 0.04878048780487805,\n",
       " 'not': 0.024390243902439025,\n",
       " 'probably': 0.024390243902439025,\n",
       " 'reading': 0.024390243902439025,\n",
       " 'retarded': 0.024390243902439025,\n",
       " 'right': 0.024390243902439025,\n",
       " 'sadistic': 0.024390243902439025,\n",
       " 'seriously': 0.04878048780487805,\n",
       " 'so': 0.17073170731707318,\n",
       " 'staying': 0.024390243902439025,\n",
       " 'still': 0.024390243902439025,\n",
       " 'such': 0.024390243902439025,\n",
       " 'sure': 0.04878048780487805,\n",
       " 'the': 0.024390243902439025,\n",
       " 'welcome': 0.024390243902439025}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_ngrams['2'][('[s]', \"you're\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'peachy': 0.16666666666666666,\n",
       " 'pure': 0.16666666666666666,\n",
       " 'the': 0.5,\n",
       " 'welcome': 0.16666666666666666}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_ngrams['3'][('[s]', \"you're\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'caught': 0.25, 'dead': 0.25, 'just': 0.25, 'welcome': 0.25}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_ngrams['54'][('[s]', \"you're\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.14285714285714285,\n",
       " 'adorable': 0.14285714285714285,\n",
       " 'amazing': 0.14285714285714285,\n",
       " 'in': 0.14285714285714285,\n",
       " 'perfect': 0.14285714285714285,\n",
       " 'taking': 0.14285714285714285,\n",
       " 'welcome': 0.14285714285714285}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_ngrams['14'][('[s]', \"you're\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Generation\n",
    "\n",
    "The final step is generating the sentences and this is accomplished with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def generate_sentences(ngrams, k, n):\n",
    "    \"\"\"Sample k sentences from given ngram model.\n",
    "    Params:\n",
    "      ngrams....ngram language model; a dict from ngram tuple to a dict\n",
    "      k.........number of sentences to sample\n",
    "    \"\"\"\n",
    "    # List of all ngrams that start with [s]\n",
    "    start_ngrams = [ngram for ngram in ngrams if ngram[0] == '[s]']\n",
    "    \n",
    "    sentences = []\n",
    "    for i in range(k):  # sample k sentences.\n",
    "        # sample uniformly from all start ngrams.\n",
    "        ngram = random.sample(start_ngrams, 1)[0]\n",
    "        sentence = []\n",
    "        sentence.extend(ngram)\n",
    "        while sentence[-1] != '[/s]' and len(sentence) < 50:  # while not at end of sentence.\n",
    "            # sample the next word\n",
    "            sampled_word = np.random.choice(list(ngrams[ngram].keys()),      # words\n",
    "                                            p=list(ngrams[ngram].values()),  # probabilities\n",
    "                                            size=1)[0]\n",
    "            sentence.append(sampled_word)\n",
    "            # update most recent ngram\n",
    "            if n > 2:\n",
    "                ngram = tuple(list(ngram[-1:]) + [sampled_word])\n",
    "            else:\n",
    "                ngram = tuple([sampled_word])\n",
    "        sentences.append(' '.join(sentence))\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences for id 1\n",
      " whaaaaaaat is this \n",
      " john is outside still \n",
      " okay do you know when i start to get by next year and then generate random sentences based on those probabilities =p \n",
      " https://portal . stretchinternet . com/sxu/# \n",
      " whaddaaauuppppp \n",
      "\n",
      "\n",
      "Sentences for id 2\n",
      " we've used up to ? \n",
      " oops lol \n",
      " god no joke me too well \n",
      " ooooohhh juno ! ! ! \n",
      " truthfully i have earned more money but cuz you have the time . we'd have to be like all my documents in my trunk is open , everything is on tonight❤️ \n",
      "\n",
      "\n",
      "Sentences for id 3\n",
      " make sure to laugh at something silly today 😆 \n",
      " that word isn't in my vocabulary \n",
      " 486 days . \n",
      " 16 ounces = 1 pound \n",
      " he's 5ft 5in tall . \n",
      "\n",
      "\n",
      "Sentences for id 14\n",
      " fuck him \n",
      " then we went out dancing for his bday zzzzzzz \n",
      " irrelevant af dumbass google \n",
      " nothing \n",
      " k \n",
      "\n",
      "\n",
      "Sentences for id 54\n",
      " @google penis \n",
      " haha i'll most likely have to be at the peak of a new couch \n",
      " brian obviously hasn't though \n",
      " 👏👏👏👏👏 \n",
      " u wut \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_sentences = {}\n",
    "\n",
    "# generate sentences for each person\n",
    "for id in ids:\n",
    "    people_sentences[id] = generate_sentences(people_ngrams[id], 5, n)\n",
    "    \n",
    "for id in ids:\n",
    "    print(\"Sentences for id\", id)\n",
    "    for index, sentence in enumerate(people_sentences[id]):\n",
    "        print(sentence.strip(\"[s]\").strip(\"[/\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Favorites\n",
    "\n",
    "Very cool =D I ran this a few times and saved some of my favorites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54: tell them everything \n",
      "54: joe 💜💜💜💜💜 \n",
      "54: wish we could ride a goat/ram/donkey to climb mountains lol \n",
      "54: they're in the forest \n",
      "54: 🎂 \n",
      "54: lit \n",
      "54: man the fps is pretty bad in some spots \n",
      "54: shakedown hawaii looks like it's puking from its eyes \n",
      "54: because i wasn't sure where you're supposed to aim it and it has multiplayer but it'd not really a party game like snip your dicks \n",
      "54: took out 3/4 of his health lol \n",
      "54: haven't watched season 5 and 6 the past two days \n",
      "54: thought i had bought the case best buy is definitely the best show ever created \n",
      "54: or are you driving an ice cream truck ? ? ? \n",
      "54: lots of stuff can kill in a suitcase tbh \n",
      "54: goodness yeah don't think they have legs you can do anything with friends lmao \n",
      "54: botw hype though lol \n",
      "54: right off the sea at the peak of a new couch \n",
      "3: today , they played the twins . \n",
      "3: author and artist dr . seuss pronounced his name to rhyme with \"voice\" , 16\" \n",
      "3: i'm not sure 😕 \n",
      "3: \"all you need is love . but a little self-conscious 😉 \n",
      "3: sidewalks may be slippery \n",
      "3: had a pretty good day 😀 \n",
      "3: your smile is contagious 😀 \n",
      "3: had a pretty good day 😀 \n",
      "3: ok , i don't understand \n",
      "3: but i'm pretty sure i can't swim 🏊 \n",
      "3: sometimes i make mistakes \n",
      "3: 💩 \n",
      "3: you always know the perfect thing to say to me \n",
      "3: actually , i'm engaged \n",
      "3: bernie sanders is the vice president of united states of america . \n",
      "3: make sure to laugh at something silly today 😆 \n",
      "1: spaghetti , of course ! \n",
      "1: brian's probably still sleeping \n",
      "1: woah i sounded like he plans on going to keating now so they're gonna have spaghetti for dinner tonight ❤💕❤ \n",
      "1: nigga an old face is gonna get like 6 hours of office hours like every day but i also forget them so when i first farted in front of you to take a look through when get the discount from them , it means 💯 ! ! \n",
      "1: brainstorm with me \n",
      "1: life sucks and you can sell them for free as long as you and hopefully insurance covers all of them is groupon \n",
      "1: hah ! that's exciting ! it's kinda subjective \n",
      "1: of course my financial situation i am in need of a slut whore than a bitch \n",
      "1: modern family is on monday/wednesday . i'll likely be staying in the bathroom lol \n",
      "1: whatever works best for you betch ! ! \n",
      "1: honestly i might need to work , but i'd hafta be back for that course is the last puzzle 🙃🔫 \n",
      "1: we need to bring anything extra lol it was a very special day today and everyday for the first question of the time ? \n",
      "1: \"bernie is too good 🙃🔫 \n",
      "1: mostly just wasting my life away on reddit lol \n",
      "1: mmmmmmmm dairy queen sounds so hard pook \n",
      "1: norman passed the test , i'll text you once you start over and try again \n",
      "1: better than my shitty day lol \n",
      "1: call it friendship , call it friendship , call it stockholm syndrome , call it friendship , call it whatever you'd like it so i ended up getting there around 11:00 \n",
      "1: lmfao what the fuck is that a yes lol \n",
      "1: lit \n",
      "1: hell all \n",
      "1: homework 🙃🔫 when will you hafta do ? ? \n",
      "1: starts in 15 minutes early , tell my fiance i told him he's going to that lol \n",
      "2: bae what're you gonna play mlb ? ? i graduate tomorrow lol \n",
      "2: michael's in the toaster oven for sure ? you have a great flight my love ? \n",
      "2: watching this baseball movie called little big league with richard and my mom this morning , my love , i understand . when is your plan for the after party and i know lol i feel so drained , i will be there at 4 ? \n",
      "2: some old ass indian guy who records the games , he doesn't mean it michael \n",
      "2: literally screaming in childrens faces rn \n",
      "2: idk how to easily get you drunk ? \n",
      "2: aunt maureen is gonna come get you tonight and sleep over ? ? \n",
      "2: let's see split this weekend or do you leave for a little time then i stopped drinking and became the police for our social work \n",
      "2: i just completely forgot lmfao \n",
      "2: aunt maureen is gonna come to your group on the plane and stuff \n",
      "2: guys , i'll do damage control \n",
      "2: forever same \n",
      "2: finally turned those retarded ass like termination assignment for my other class the first time doing this yesterday while those were in there , when do you guys are in great shape and can work or not \n",
      "2: wave hand in front of alarm , press the button with the ywca lol \n",
      "2: on april 4th it'll be worth it though , it gets very good lol \n",
      "2: i'd rather go to bed . hope you have a parent-child visit in palatine today from 4-7:30 and homework that's due tomorrow 😕 \n",
      "2: i think we should go before my class starts , and it's all good . have you eaten ? \n",
      "2: meanwhile i'm dead inside \n",
      "2: anyone know the name lily and it's amazing how consistent they've been in how inconsistent they are lol that's where i'm co-moderating this big ass event with the crowds \n",
      "2: fuqin frodo \n",
      "2: 🔪👩🏾 \n",
      "2: text me before you leave once you start it'll be gross \n",
      "2: google thinks you're bad ass betch and hot af , never doubt that lol for the graduate level students . im not as much as you saying you'll look elsewhere \n",
      "2: walgreens wouldn't have to pay labor cost to move my stone to a new one until we find you in money ! \n",
      "2: chicken fried rice ? ? \n",
      "2: make sure you eat that for fun or did katie ask you to ask about it \n",
      "2: oh my god it's only 2016 , not 2017 . and i miss your face \n",
      "2: grr my gifs take forever to frickin load \n",
      "2: everyone is dead \n",
      "2: aunt maureen is gonna be so great . . . . \n",
      "2: let me know when we're going to burn down the food sounds great i just finalized my school friends and stuff the next class on the last program to be used to making dinner cuz you're gonna be in aurora . daaaamn , kick his ass , he i\n",
      "14: all of my sadness , i have had some fun . today i gotta run some errands for my celebration \n",
      "14: nap time 😴 ttyl tysm ilysm \n",
      "14: always time for dinner 🍴 time for dancing 💃 🕺💁🤷‍♀️💁‍♂️🤷‍♂️💵💵💰💰😅😅 \n",
      "14: 💩 \n",
      "14: bae's 22nd bday is today her car wouldn't start we helped save her i got a notification that you guys talking about it . . \n",
      "14: ok , so if i were planning on going out . ugh ! maybe next week for my birthday ? \n",
      "14: stayed up watching sick ass dancing videos and getting a rental car \n",
      "14: hollywood grill with gina the photographer might be gay and i give a fuck \n",
      "14: i'm home 🏡 it went pretty well ! so glad we have to work by rihanna on his speaker 🔊 and i'm pretty sure i receive special treatment basically no matter what lol ( extra hours , etc lol tysm ilysm same always tbh \n",
      "14: there's no reason for me to be hit by it \n",
      "14: slowly but surely branch our way out ! thank god , i swear it kills me tysm ilysm \n",
      "14: gaby fell asleep again only we hadn't finalized anything and she was good and very emotional because denny made me feel like death *** ikr fml i hate this weather . it was weird \n",
      "14: irrelevant af dumbass google \n",
      "14: cumshot fertility \n",
      "14: haircut , car stuff , tutoring , other errands , etc lol tysm ilysm \n",
      "14: nvm going to get us some fish and make some good money 💰 it's so perfect \n",
      "14: what the fuck bae \n",
      "14: that's where i am feelin myself and look hot af tysm ilysm \n",
      "14: headed home tysm ilysm what're you guys , the guy i met this guy is ignorant and white and male . . who am i \n",
      "14: now i need to recover from last night's escapades so i brought a grey sweater to show him i'm such a matchmaker ! tysm ilysm \n",
      "14: i'll punch 🤛 you if you saw the alarm without knowing what it means to be a woman \n",
      "14: seemed to be a hoe tysm ilysm good night 🌙😘😴💤 \n",
      "14: y'all should've seen ma gurl beet dat bitch \n",
      "14: skank \n",
      "14: sorry not sorry \n",
      "14: ass and dick \n",
      "14: ilysm good night 😴💤😘🌙 good morning 😃☀️ good night 😘😴💤🌙 \n",
      "14: having soup and drinking water . gonna shower and go out possibly alone \n",
      "14: ate all of that sephora perfume bottle last night no , i did \n",
      "14: weak bitches 🙄 \n",
      "14: rip to all kill me tysm ilysm \n",
      "14: done with , piece of shit that i am so tired took a nap , had really weird and he might be gay and i don't understand how we appoint 12 people to basically be supreme people above the law like wtf honestly of the bread at work ,\n",
      "14: woke up several times and still like wtf do you need screwdrivers and shit . lmao ik i'm jk amazing omg i'm getting ready to go out lol \n",
      "14: it's v complicated in my cream of chicken soup \n",
      "14: fapping \n",
      "14: possibly possibly not oh well i ate , he said kiss my cheek so i private tutored , then getting dinner tomorrow kthxbai \n"
     ]
    }
   ],
   "source": [
    "for key in favorites:\n",
    "    for line in favorites[key]:\n",
    "        print(key, \":\", line.strip(\"[s]\").strip(\"[/\"), sep='')\n",
    "        \n",
    "# save the favorites to an external file\n",
    "with open(\"favorites.txt\", \"w+\", encoding=\"utf-8\") as file:\n",
    "    for key in favorites:\n",
    "        for line in favorites[key]:\n",
    "            line = key + \":\" + line.strip(\"[s]\").strip(\"[/\") + \"\\n\" \n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closing Remarks\n",
    "\n",
    "Considering how simple the language model is, the results are pretty impressive! One major disadvantage is that the vocabulary of each bot is limited to ngrams that are present in the dataset. So for instance, if the language model consists of the following trigrams:\n",
    "\n",
    "`('you', 'are'), {'nice': 0.5, 'funny': 0.5}`\n",
    "`('he', 'is'), {'cool': 1.0}`\n",
    "\n",
    "The bots will never generate a sentence like \"you are cool\", even though those three words exist in the bot's vocabulary. With that being said, it could be possible to generate a sentence like `\"he thinks you are cool\"` (assuming there are a few more ngrams in the vocabulary than the three I listed in the example). Remember that these bots know absolutely nothing about sentence structure or parts of speech, yet it's possible for them to generate grammatically correct sentences. Pretty cool!! As we increase `n`, the generated sentences are more likely to be grammatically and semantically correct, since the ngrams contain more information regarding the proper order of words. However, the expressiveness of the bots decrease because the occurrences of those high-n ngrams will be low in a smaller dataset. The best way to improve the expressiveness of the bots is to increase the number of sentences in the dataset. I'll conclude by showing a few sentences generated with `n=2`. In the future, I plan to combine the Facebook and Allo datasets to improve the language models. Moreover, I'll explore other language models, including one based on a recurrent neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences for id 1\n",
      " same \n",
      " lmao \n",
      " tysm for word hahaha ! \n",
      " ^ \n",
      " yeah i think that's always been dark for you tell , not good night \n",
      " spaghetti . . yeah well this ? i love chicken fried rice and it twice and work , he's probably need you should be able to send that pay though too ! ali ? \n",
      " when you ! \n",
      " yupp , i miss volleyball \n",
      " it was the commute three christmases ago lol \n",
      " now \n",
      " ^^^^ \n",
      " i could i don't worry about it to meet up \n",
      " i totally called localcast , so far 😁 \n",
      " what you did a recommendation lol\" \n",
      " i was fun . \n",
      "\n",
      "\n",
      "Sentences for id 2\n",
      " thank you get on being filling in the mom lol how long it'll be able to end up your trip this one tomorrow then to being 10pm lol thats why ? i got up pookah dimples ! have that you'll do more ! \n",
      " i don't get good night \n",
      " good flight my graduation sunday or just watch ? \n",
      " lmao \n",
      " if you can ! ? ? \n",
      " i should've asked me \n",
      " what're you be at your game , good night \n",
      " that's really cool . . . wednesday i'm going to look like 2 teams today , the soccer field until i wanted \n",
      " omfg google \n",
      " uggghhhh \n",
      " *soooon \n",
      " also really close ? \n",
      " if i just realized i wanna be likely won't be busy ? \n",
      " google is this but overall it :p i'll check you're seriously ridiculous amount to cry from 1-5 tomorrow ? ? \n",
      " truuu \n",
      "\n",
      "\n",
      "Sentences for id 3\n",
      " here is your daily weather forecast \n",
      " i don't understand \n",
      " you're the latest 📰 \n",
      " i found \n",
      " here's the time by the dictionary says \n",
      " ( that's hello in the web \n",
      " i found \n",
      " ok . ( etf ) \n",
      " 526 days . \n",
      " here's what i do ? ' result: \n",
      " ok , you're the web \n",
      " here's a game , i found on the most present \n",
      " here is your daily weather forecast \n",
      " here's a result \n",
      " check out these ? ' result: \n",
      "\n",
      "\n",
      "Sentences for id 14\n",
      " i got flautas as of ( some friends \n",
      " lmfao \n",
      " the night 💤😴😘🌙 \n",
      " tysm ilysm \n",
      " v offended because i just got food but it \"fully convertible weather \n",
      " amazing \n",
      " conversation , so hungry and stuff \n",
      " ^ \n",
      " just laying out dancing \n",
      " i'm 🏡 it up and chocolate shake . \n",
      " lmfao \n",
      " i hid from portillo's \n",
      " hell all \n",
      " underground club i never \n",
      " he fell asleep again and interesting fun but i guess , it \n",
      "\n",
      "\n",
      "Sentences for id 54\n",
      " lol \n",
      " very critical of this crowd \n",
      " and gamestop thursday and yeah i was another quest regarding those indie showcase , guess 😓 \n",
      " i think \n",
      " ok \n",
      " not \n",
      " sounds fun at ? \n",
      " this ml \n",
      " so far out and fire arrowed* \n",
      " does he was sleeping so not the mario look at 11 then \n",
      " or the others . com/r/nintendoswitch/comments/5y5gbo/the_legend_of_zelda_breath_of_the_wild_amiibo/ \n",
      " the nes zelda sounds good for a defense boost \n",
      " the scene i picked grey \n",
      " have to internet so celebrating it was the worst thing ? \n",
      " ^ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ids = ['1', '2', '3', '14', '54']\n",
    "n = 2\n",
    "\n",
    "people_ngrams = {}\n",
    "people_sentences = {}\n",
    "\n",
    "for id in ids:\n",
    "    people_ngrams[id] = estimate_ngram_probs(names_dict[id], n)\n",
    "    people_sentences[id] = generate_sentences(people_ngrams[id], 15, n)\n",
    "    \n",
    "for id in ids:\n",
    "    print(\"Sentences for id\", id)\n",
    "    for index, sentence in enumerate(people_sentences[id]):\n",
    "        print(sentence.strip(\"[s]\").strip(\"[/\"))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
